defaults:
  - _self_
  - model: dit-l-2
  - data: dummy256    # dummy data
  - autoencoder: tiny_ae
  - lr_scheduler: null
  - experiment: null    # must be last in defaults list as it can override all others

  # disable hydra logging
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled
  
# ----------------------------------------
name: debug/your_exp

# ----------------------------------------
# logging
use_wandb: False
use_wandb_offline: False
wandb_project: image-ldm

tags: []

# checkpoint loading
load_weights: null
load_strict: True
resume_checkpoint: null

# checkpoint saving (lightning callback)
checkpoint_params:              # filename refers to number of gradient updates
    every_n_train_steps: 10000  # gradient update steps
    save_top_k: -1              # needs to be -1, otherwise it overwrites
    verbose: True
    save_last: True
    auto_insert_metric_name: False

# ----------------------------------------
# train logics
trainer_module:
  target: ldm.trainer.TrainerModuleLatentFlow
  params:
    # model specific
    model: ${model}
    flow_cfg:
      target: ldm.flow.Flow
    first_stage_cfg: ${oc.select:autoencoder, null}
    # learning
    lr: 1e-4
    weight_decay: 0.0
    ema_rate: 0.9999       # if 0, no EMA
    lr_scheduler_cfg: ${oc.select:lr_scheduler, null}
    # logging
    log_grad_norm: False      # might be slow
    n_images_to_vis: 16
    sample_kwargs:
      num_steps: 50
      progress: False

# ----------------------------------------
trainer_params:
  max_steps: -1
  max_epochs: -1
  num_sanity_val_steps: 0
  accumulate_grad_batches: 1
  log_every_n_steps: 50       # gradient update steps
  limit_val_batches: 8        # per GPU
  val_check_interval: 5000    # steps, regardless of gradient accumulation
  precision: bf16-mixed

callbacks:
  - target: lightning.pytorch.callbacks.LearningRateMonitor
    params:
      logging_interval: 'step'

# ----------------------------------------
# profiling
profile: false
profiling:
  warmup: 40
  active: 1
  filename: profile.json
  cpu: true
  cuda: true
  record_shapes: false
  profile_memory: false
  with_flops: false

# ----------------------------------------
# distributed
num_nodes: 1
devices: -1
auto_requeue: False
tqdm_refresh_rate: 1        # set higher on slurm (otherwise prints tqdm every step)
deepspeed_stage: 0
p2p_disable: False          # heidelberg
slurm_id: null

# ----------------------------------------
user: ${oc.env:USER}

# don't log and save files
hydra:
  output_subdir: null
  run:
    dir: .
