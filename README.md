## Master Practical of Image and Video Synthesis  <br><sub>**Research of Diversity of SiT**</sub>
[Our Report Paper](https://arxiv.org/pdf/2401.08740.pdf)

![SiT samples](examples/odevssdeâ€”cfg/ode_sde_cfg.png)

This repository extends the original [Scalable Interpolant Transformer (SiT)](https://arxiv.org/abs/2401.08740) project to **systematically evaluate the diversity** of generated images using various perceptual and statistical metrics. Our practical focuses on **benchmarking and analyzing SiT under different sampling settings** (SDE/ODE, CFG, noise levels, etc.), and includes tools for evaluating:

- **FID / sFID**
- **Inception Score**
- **Precision & Recall**
- **LPIPS (perceptual diversity)**
- **DreamSim Distance**
- **DINOv2 Similarity**
- **CLIP-based Feature Diversity**

## âœ… Project Goals

**This project aims to systematically evaluate the diversity and quality of images generated by the Scalable Interpolant Transformer (SiT). Specifically, we seek to answer the following questions:**

1. How do different **sampling configurations (cfg values)** affect the diversity and quality of SiT-generated samples?
2. How do **sampling methods** (ODE vs. SDE) influence diversity and quality?
3. How does the choice of **SDE schedule and diffusion norm** impact the diversity of the generated outputs?
4. Do different **diversity metrics** yield consistent evaluations of sample diversity?

   - **4.1 ViT-based metrics**:  
     How consistent are evaluations from metrics that rely on Vision Transformer backbones, such as:
     - CLIP-based diversity
     - DINOv2-based diversity
     - DreamSim distance

   - **4.2 ResNet-based metrics**:  
     How consistent are evaluations from metrics that use ResNet-style architectures, including:
     - LPIPS
     - MoCo-based diversity
     - SimCLR-based diversity

To achieve this, we implemented a comprehensive evaluation pipeline that measures both **intra-class** and **inter-class** diversity.


---
## ðŸ“‚ Folder Structure

<summary><strong>ðŸ“‚ Folder Structure</strong></summary>

```text
â”œâ”€â”€ SiT         # Original Implementation of SiT
â”œâ”€â”€ diversity_metrics/      
â”‚   â”œâ”€â”€ compute_fid.py  
â”‚   â”œâ”€â”€ eval.py  
â”‚   â””â”€â”€ metrics.py   
â”‚   
â”‚â”€â”€ exp-local/      
â”‚   â”œâ”€â”€ cfg_sde&ode_res/
â”‚   â”‚   â”œâ”€â”€ compute_fid.py  
â”‚   â”‚   â”œâ”€â”€ eval.py  
â”‚   â”‚   â””â”€â”€ metrics.py   
â”‚   â”‚
â”‚   â”œâ”€â”€ sde_norm&form/ 
â”‚   â”‚   â”œâ”€â”€ compute_fid.py  
â”‚   â”‚   â”œâ”€â”€ eval.py  
â”‚   â”‚   â””â”€â”€ metrics.py  
â”‚ 
â”œâ”€â”€ validation_loss/               
â””â”€â”€ vis/               
        
```
---


## ðŸ”§ Setup

Clone the repo and create the environment:

```bash
git clone https://github.com/77Han329/CVPractical.git
cd CVPractical
conda env create -f SiT/environment.yml
conda activate SiT

# Generate samples across multiple classes
torchrun --nproc_per_node=4 sample_ddp.py ODE --model SiT-XL/2 --num-fid-samples 10000

# Compute metrics from saved samples(FID,sFID,Inception Score, Precision, Recall)
python diversity_metrics/compute_fid.py \
  --ref_batch samples/VIRTUAL_imagenet256_labeled.npz \
  --sample_batch path/to/sample_batch.npz

##Visualizing Diversity
python vis.py --csv-dir --save-dir outputs/
```
---

## Experiment Results
[Download SiT Samples (CFG=1.0, ODE)](https://github.com/77Han329/CVPractical/releases/download/v1-samples/sit_samples_cfg1.0_ode_seed250.zip)

## ðŸ“¦ Pre-Sampled Outputs

We provide pre-generated sample outputs of the SiT-XL/2 model for reproducibility and metric evaluation.

ðŸ‘‰ [Download Sample Output (CFG=1.0, ODE)](https://github.com/77Han329/CVPractical/releases/download/sit-samples-v1/SiT-XL-2-pretrained-cfg-1.0-4-ODE-250-euler.npz.zip)

- Model: `SiT-XL/2`
- Config: `CFG=1.0`
- Sampler: `ODE`
- Format: `.npz`
- Size: 153 MB
